{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TokenizingPadding.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMxGgewAXnd85ykZT0EEQ7C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/happyrabbit/IntroDataScience/blob/master/Python/TokenizingPadding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqUWz7TYKsLp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load packages\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISZqTXTAG2kE",
        "colab_type": "text"
      },
      "source": [
        "We give each word a value and use those values to train a neural network. Consider the following review sentences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__pxtJR3oUMD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = ['This movie is great!',\n",
        "'Great movie ? Are you kidding  me ! Not worth the money.',\n",
        "'Love it']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qcf9agvSGRA",
        "colab_type": "text"
      },
      "source": [
        "We set `num_words = 100` which is the number of distinct words. It is way too big for this baby example. If you're creating a training set based on lots of text, you usually don't know how many distinct words there are in the text. So by setting this hyperparameter, the tokenizer will take the top 100 words by volume and just encode those. It's a handy shortcut when dealing with lots of data, and worth experimenting with when you train with real data. Sometimes the impact of less words can be minimal on training accuracy, but huge on training time.\n",
        "\n",
        "`oov_token = \"<oov>\"` specifies the token for outer vocabulary to be used for words that aren't in the word index. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZoeg5Z8K9D1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(num_words= 100, oov_token= \"<oov>\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmiZ5n9GS0xR",
        "colab_type": "text"
      },
      "source": [
        "`fit_on_texts` method of the `tokenizer` encodes the sentences. The tokenizer has a `word_index` property which returns a dictionary containing key value pairs, where the key is the word, and the value is the token for that word. You can inspect by simply printing it out. Note that the tokenizer lower-cases all words and strips punctuation out. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2LXQjHbS3Wc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# call the tokenizer to get texts to sequences\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "padded = pad_sequences(sequences, maxlen=6, padding = \"post\", truncating = 'post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baDi5t2AQ2ff",
        "colab_type": "text"
      },
      "source": [
        "![](https://course2020.scientistcafe.com/slides/02DeepLearning/RNN/images/TokenizingPadding.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auCW1Qnpovl2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "421aae75-6966-47c5-9f71-17f078285b68"
      },
      "source": [
        "print(\"\\nWord Index = \", word_index)\n",
        "print(\"\\nSequences = \", sequences)\n",
        "print(\"\\nPadded Sequences:\")\n",
        "print(padded)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Word Index =  {'<oov>': 1, 'movie': 2, 'great': 3, 'this': 4, 'is': 5, 'are': 6, 'you': 7, 'kidding': 8, 'me': 9, 'not': 10, 'worth': 11, 'the': 12, 'money': 13, 'love': 14, 'it': 15}\n",
            "\n",
            "Sequences =  [[4, 2, 5, 3], [3, 2, 6, 7, 8, 9, 10, 11, 12, 13], [14, 15]]\n",
            "\n",
            "Padded Sequences:\n",
            "[[ 4  2  5  3  0  0]\n",
            " [ 3  2  6  7  8  9]\n",
            " [14 15  0  0  0  0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4iqQTZGo1TF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qyu68V5vpc9-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "863a9173-4709-4bb6-e9d4-a62718a31d55"
      },
      "source": [
        "def decode_review(text):\n",
        "  return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "print(decode_review(padded[1]))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "great movie are you kidding\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKtg9Iw3phRI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}