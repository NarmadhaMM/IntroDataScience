{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MINST-with-one-hidden-layer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOOAMTZkt65yOvEQI0mWVuX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/happyrabbit/IntroDataScience/blob/master/Python/MINST_with_one_hidden_layer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIstG9VsPyb2",
        "colab_type": "text"
      },
      "source": [
        "# Packages\n",
        "\n",
        "Import packages for the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e5WqQGj2tlo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-m2iScR3RvE",
        "colab_type": "text"
      },
      "source": [
        "# Load Data\n",
        "\n",
        "Let's load the MINST data set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgbepWh429f2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "img_width = X_train.shape[1]\n",
        "img_height = X_train.shape[2]\n",
        "\n",
        "# one hot encode outputs\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTRwGbNJ4Faj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "72d75d91-0ec1-4eb2-8492-e6c62f097a25"
      },
      "source": [
        "print('The shape of X_train is: ' + str(X_train.shape)) \n",
        "print('The shape of y_train is: ' + str(y_train.shape)) \n",
        "print('The shape of X_test is: ' + str(X_test.shape)) \n",
        "print('The shape of y_test is: ' + str(y_test.shape))\n",
        "print('I have %d traning and %d testing examples!' %(X_train.shape[0], X_test.shape[0])) "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shape of X_train is: (60000, 28, 28)\n",
            "The shape of y_train is: (60000, 10)\n",
            "The shape of X_test is: (10000, 28, 28)\n",
            "The shape of y_test is: (10000, 10)\n",
            "I have 60000 traning and 10000 testing examples!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qq1P18bhWT3A",
        "colab_type": "text"
      },
      "source": [
        "Check some of the input images "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hsXDMtEWc-C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "83e1e18d-803f-4cf6-e179-06842eb8de77"
      },
      "source": [
        "# assign the number of the image to show\n",
        "img_id = 10\n",
        "plt.imshow(X_train[img_id], cmap='gray')\n",
        "print(\"Label: \"+ str(np.where(y_train[img_id] ==1)[0]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label: [3]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZH2gGO_yRfW4",
        "colab_type": "text"
      },
      "source": [
        "Scale and reshape the inputs and outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "og6T69tsP-OD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "bc70828a-acac-47a6-e42d-027cf1aae8fa"
      },
      "source": [
        "# Scale the input\n",
        "X_train = X_train / 255.\n",
        "X_test = X_test / 255.\n",
        "\n",
        "# Flatten the input\n",
        "X_train = np.array([X_train[i].flatten() for i in range(0,X_train.shape[0])])\n",
        "X_test = np.array([X_test[i].flatten() for i in range(0,X_test.shape[0])])\n",
        "\n",
        "# Reshape the input and output\n",
        "X_train = X_train.T\n",
        "X_test = X_test.T\n",
        "y_train = y_train.T\n",
        "y_test = y_test.T\n",
        "\n",
        "print('The shape of X_train is: ' + str(X_train.shape)) \n",
        "print('The shape of y_train is: ' + str(y_train.shape)) \n",
        "print('The shape of X_test is: ' + str(X_test.shape)) \n",
        "print('The shape of y_test is: ' + str(y_test.shape))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shape of X_train is: (784, 60000)\n",
            "The shape of y_train is: (10, 60000)\n",
            "The shape of X_test is: (784, 10000)\n",
            "The shape of y_test is: (10, 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfufnet5VBPQ",
        "colab_type": "text"
      },
      "source": [
        "# One Hidden Layer Neural Network\n",
        "<center><img src=\"https://raw.githubusercontent.com/happyrabbit/course2020/master/slides/02DeepLearning/DNN/images/1hiddenexp.png\" width=\"700\"/></center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPgV2WF7Do4I",
        "colab_type": "text"
      },
      "source": [
        "# Defining the neural network structure\n",
        "\n",
        "- `n_x` -- the size of the input layer\n",
        "- `n_h` -- the size of the hidden layer\n",
        "- `n_y` -- the size of the output layer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa4lvy8dCx8B",
        "colab_type": "text"
      },
      "source": [
        "#  Initialize the model's parameters\n",
        "\n",
        "- Initialize the weights matrices, $W_1$ and $W_2$, with random values.\n",
        "- Initialize the bias vectors, $b_1$ and $b_2$, as zeros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGbe96XiQhW2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_parameters(n_x, n_h, n_y):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    n_x -- size of the input layer\n",
        "    n_h -- size of the hidden layer\n",
        "    n_y -- size of the output layer\n",
        "    \n",
        "    Returns:\n",
        "    params -- python dictionary containing your parameters:\n",
        "                    W1 -- weight matrix of shape (n_h, n_x)\n",
        "                    b1 -- bias vector of shape (n_h, 1)\n",
        "                    W2 -- weight matrix of shape (n_y, n_h)\n",
        "                    b2 -- bias vector of shape (n_y, 1)\n",
        "    \"\"\"\n",
        "        \n",
        "    W1 = np.random.randn(n_h, n_x) * 0.001\n",
        "    b1 = np.zeros(shape=(n_h, 1))\n",
        "    W2 = np.random.randn(n_y, n_h) * 0.001\n",
        "    b2 = np.zeros(shape=(n_y, 1))\n",
        "    \n",
        "    assert (W1.shape == (n_h, n_x))\n",
        "    assert (b1.shape == (n_h, 1))\n",
        "    assert (W2.shape == (n_y, n_h))\n",
        "    assert (b2.shape == (n_y, 1))\n",
        "    \n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Duea8jLXHMEm",
        "colab_type": "text"
      },
      "source": [
        "# Defining Activation Functions and the Gradients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbupxz65Hm6S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ReLU(Z1):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    Z1 -- the input of hidden layer activation function\n",
        "    \n",
        "    Returns:\n",
        "    A1 -- activations from the hidden layer\n",
        "    \"\"\"\n",
        "    A1 = np.maximum(Z1, 0)\n",
        "    return A1\n",
        "\n",
        "def ReLU_dev(Z1):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    Z1 -- the input of hidden layer activation function\n",
        "    \n",
        "    Returns:\n",
        "    dZ1 -- gradient of ReLU given input Z1\n",
        "    \"\"\"\n",
        "    positive = Z1 > 0\n",
        "    dZ1 = np.zeros(Z1.shape)\n",
        "    dZ1[positive] = 1\n",
        "    return dZ1\n",
        "\n",
        "def softmax(Z2):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    Z2 -- the input of output layer activation function\n",
        "    \n",
        "    Returns:\n",
        "    A2 -- activations from the hidden layer\n",
        "    \"\"\"\n",
        "    exp_scores = np.exp(Z2 - Z2.max(axis = 0, keepdims=True))\n",
        "    A2 = exp_scores / exp_scores.sum(axis = 0, keepdims=True)\n",
        "    return A2"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkS9OaatW8NF",
        "colab_type": "text"
      },
      "source": [
        "# Loss and Accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8Cskf4wWdcH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_cost(A2, Y, parameters):\n",
        "    \"\"\"\n",
        "    Computes the cross-entropy cost given in equation (13)\n",
        "    \n",
        "    Arguments:\n",
        "    A2 -- The sigmoid output of the second activation, of shape (10, number of examples)\n",
        "    Y -- \"true\" labels vector of shape (10, number of examples)\n",
        "    parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n",
        "    \n",
        "    Returns:\n",
        "    cost -- cross-entropy cost given equation (13)\n",
        "    \"\"\"\n",
        "    m = Y.shape[1] # number of example\n",
        "    # Retrieve W1 and W2 from parameters\n",
        "    W1 = parameters['W1']\n",
        "    W2 = parameters['W2']\n",
        "    \n",
        "    # Compute the cross-entropy cost\n",
        "    logprobs = np.multiply(np.log(A2), Y)\n",
        "    cost = - np.sum(logprobs) / m\n",
        "    \n",
        "    cost = np.squeeze(cost)     # makes sure cost is the dimension we expect. \n",
        "                                # E.g., turns [[17]] into 17 \n",
        "    assert(isinstance(cost, float))\n",
        "    return cost"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4hoXxx9FLw4",
        "colab_type": "text"
      },
      "source": [
        "# Forward Propagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujmmyE0CEysi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_propagation(X, parameters):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    X -- input data of size (n_x, m)\n",
        "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
        "    \n",
        "    Returns:\n",
        "    A2 -- The sigmoid output of the second activation\n",
        "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
        "    \"\"\"\n",
        "    # Retrieve each parameter from the dictionary \"parameters\"\n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "    \n",
        "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
        "    Z1 = np.dot(W1, X) + b1\n",
        "    A1 = ReLU(Z1)\n",
        "    Z2 = np.dot(W2, A1) + b2\n",
        "    A2 = softmax(Z2)\n",
        "    \n",
        "    assert(A2.shape == (10, X.shape[1]))\n",
        "    \n",
        "    cache = {\"Z1\": Z1,\n",
        "             \"A1\": A1,\n",
        "             \"Z2\": Z2,\n",
        "             \"A2\": A2}\n",
        "    \n",
        "    return A2, cache"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdQpBDdnaQIO",
        "colab_type": "text"
      },
      "source": [
        "# Backward Propagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8KONXGME3Y_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backward_propagation(parameters, cache, X, Y):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation using the instructions above.\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing our parameters \n",
        "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
        "    X -- input data of shape (784, number of examples)\n",
        "    Y -- \"true\" labels vector of shape (10, number of examples)\n",
        "    \n",
        "    Returns:\n",
        "    grads -- python dictionary containing your gradients with respect to different parameters\n",
        "    \"\"\"\n",
        "    m = X.shape[1]\n",
        "    \n",
        "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
        "    W1 = parameters['W1']\n",
        "    W2 = parameters['W2']\n",
        "        \n",
        "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
        "    A1 = cache['A1']\n",
        "    A2 = cache['A2']\n",
        "    Z1 = cache['Z1']\n",
        "    Z2 = cache['Z2']\n",
        "    \n",
        "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
        "    dZ2= A2 - Y\n",
        "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
        "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
        "    dZ1 = np.multiply(np.dot(W2.T, dZ2), ReLU_dev(Z1))\n",
        "    dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
        "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
        "    \n",
        "    grads = {\"dW1\": dW1,\n",
        "             \"db1\": db1,\n",
        "             \"dW2\": dW2,\n",
        "             \"db2\": db2}\n",
        "    \n",
        "    return grads"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyVadBizc4Kc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_parameters(parameters, grads, learning_rate= 0.1, l2 = 0):\n",
        "    \"\"\"\n",
        "    Updates parameters using the gradient descent update rule given above\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    grads -- python dictionary containing your gradients \n",
        "    learning_rate -- a number between 0 to 1\n",
        "    l2 -- parameter for l2 penality, a number >= 0\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "    \"\"\"\n",
        "    # Retrieve each parameter from the dictionary \"parameters\"\n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "    \n",
        "    # Retrieve each gradient from the dictionary \"grads\"\n",
        "    dW1 = grads['dW1'] + np.sum(l2*W1)\n",
        "    db1 = grads['db1'] \n",
        "    dW2 = grads['dW2'] + np.sum(l2*W2)\n",
        "    db2 = grads['db2'] \n",
        "    \n",
        "    # Update rule for each parameter\n",
        "    W1 = W1 - learning_rate * dW1\n",
        "    b1 = b1 - learning_rate * db1\n",
        "    W2 = W2 - learning_rate * dW2\n",
        "    b2 = b2 - learning_rate * db2\n",
        "    \n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NON-Sf9QdFnb",
        "colab_type": "text"
      },
      "source": [
        "# Build Your Own Neural Network "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtghT07DdOCA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nn_model(X, Y, n_h, num_iterations=10000, print_cost=False, learning_rate= 0.1, l2 = 0):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    X -- dataset of shape (784, number of examples)\n",
        "    Y -- labels of shape (10, number of examples)\n",
        "    n_h -- size of the hidden layer\n",
        "    num_iterations -- Number of iterations in gradient descent loop\n",
        "    print_cost -- if True, print the cost every 1000 iterations\n",
        "    learning_rate -- a number between 0 to 1\n",
        "    l2 -- parameter for l2 penality, a number >= 0\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
        "    costs -- a list of cost per 100 interations\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(1)\n",
        "    n_x = layer_sizes(X, Y)[0]\n",
        "    n_y = layer_sizes(X, Y)[2]\n",
        "    costs = []\n",
        "    \n",
        "    # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: \"n_x, n_h, n_y\". Outputs = \"W1, b1, W2, b2, parameters\".\n",
        "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "    \n",
        "    # Loop (gradient descent)\n",
        "\n",
        "    for i in range(0, num_iterations):\n",
        "         \n",
        "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
        "        A2, cache = forward_propagation(X, parameters)\n",
        "        \n",
        "        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n",
        "        cost = compute_cost(A2, Y, parameters)\n",
        " \n",
        "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
        "        grads = backward_propagation(parameters, cache, X, Y)\n",
        " \n",
        "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
        "        parameters = update_parameters(parameters, grads, learning_rate= learning_rate, l2 = l2)\n",
        "        \n",
        "        # Print the cost every 1000 iterations\n",
        "        if print_cost and i % 1000 == 0:\n",
        "          print (\"Cost after iteration %i: %f\" % (i, cost))\n",
        "        if i % 100 == 0:\n",
        "          costs.append(cost)\n",
        "\n",
        "    return parameters, costs"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb5eD9qOduZK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "0600bc01-b4d2-4d67-c54c-4431d9fb191c"
      },
      "source": [
        "# Build a model with a n_h-dimensional hidden layer\n",
        "parameters, costs = nn_model(X_train, y_train, n_h = 4, num_iterations=10000, print_cost=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 2.302587\n",
            "Cost after iteration 1000: 0.565407\n",
            "Cost after iteration 2000: 0.499799\n",
            "Cost after iteration 3000: 0.479786\n",
            "Cost after iteration 4000: 0.466273\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmVWTypFYLI1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(parameters, X):\n",
        "    \"\"\"\n",
        "    Using the learned parameters, predicts a class for each example in X\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    X -- input data of size (n_x, m)\n",
        "    \n",
        "    Returns\n",
        "    predictions -- vector of predictions of our model\n",
        "    \"\"\"\n",
        "    \n",
        "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
        "    A2, cache = forward_propagation(X, parameters)\n",
        "\n",
        "    predictions = np.argmax(A2, axis = 0)\n",
        "    return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENpD3RL8ZWMq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}